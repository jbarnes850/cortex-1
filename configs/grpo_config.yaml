# GRPO Training Configuration for NEAR Cortex-1
# Following Unsloth's recommended parameters for optimal reasoning performance

# Model Configuration
model:
  name: "microsoft/phi-4"
  tokenizer_name: "microsoft/phi-4"
  max_length: 16384
  load_in_4bit: false  # Using full precision model
  use_flash_attention: true
  max_seq_length: 16384

# GRPO Training Parameters
training:
  # Core GRPO parameters
  generations_per_prompt: 16  # Maximum recommended for best GRPO results
  max_seq_length: 16384       # Maximum sequence length for Phi-4
  max_tokens_per_batch: 16384 # Maximum tokens per batch
  
  # Optimization parameters
  learning_rate: 5e-6
  weight_decay: 0.01
  warmup_steps: 100
  gradient_accumulation_steps: 4  # Can be reduced from 8
  max_grad_norm: 1.0
  
  # Training duration
  num_epochs: 3
  min_training_steps: 300      # Minimum steps for observable improvements
  target_training_time_hours: 12  # Recommended training time
  early_stopping_patience: 5   # Epochs without improvement before stopping
  
  # Batch sizing
  per_device_train_batch_size: 2  # Can likely be increased to 2-4
  per_device_eval_batch_size: 1
  
  # Optimization strategy
  optimizer: "adamw_torch"
  lr_scheduler_type: "cosine"
  seed: 42

# Reward Function Configuration
reward:
  # Reward scaling
  baseline_subtract: true      # Subtract baseline reward from all rewards
  normalize_rewards: true      # Normalize rewards to have mean 0 and std 1
  clip_rewards: 1.0            # Clip rewards to [-1, 1]
  
  # Reward function weights
  weights:
    financial_insight: 0.3
    reasoning_depth: 0.3
    calculation_accuracy: 0.2
    recommendation_clarity: 0.2
    confidence_interval: 0.8
    investment_insight: 1.0
    citation_format: 0.7
    structure: 0.6
    completeness: 0.8
    metric_citation: 0.9
    historical_reference: 0.7
  
  # PPO/GRPO specific
  kl_penalty: 0.05             # KL divergence penalty
  entropy_bonus: 0.0           # Entropy bonus coefficient

# Distributed Training Configuration
distributed:
  strategy: "deepspeed_stage_3"
  gradient_checkpointing: true
  zero3_init_flag: true
  offload_optimizer: true
  offload_param: false         # Use sparingly, can slow down training

# Data Configuration
data:
  train_sizes: [500, 1000, 2000] # Different dataset sizes for experimentation
  train_path: "data/splits/financial_analysis/train_{size}_20250302_0749.jsonl"
  eval_path: "data/splits/financial_analysis/eval_{size}_20250302_0749.jsonl"
  verify_quality: true          # Whether to verify the quality of examples

# Logging & Evaluation
logging:
  log_interval_steps: 10
  eval_interval_steps: 100
  checkpoint_interval_hours: 1
  log_reward_components: true   # Log individual reward components
  
  # WandB configuration
  wandb:
    enabled: true
    project: "cortex-1"
    entity: "jbarnes850-near-protocol"
    tags: ["crypto", "grpo", "financial-analysis"]
    log_model: true 