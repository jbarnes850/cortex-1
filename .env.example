# API Keys
FLIPSIDE_API_KEY="your_flipside_api_key"
OPENAI_API_KEY="your_openai_api_key"

# Model Configuration
MODEL_NAME="meta-llama/Llama-3.3-70B-Instruct"
MAX_TOKENS=2000
TEMPERATURE=0.7

# Training Configuration
WANDB_API_KEY="your_wandb_api_key"  # for experiment tracking
HUGGINGFACE_TOKEN="your_hf_token"    # for model downloads

# Infrastructure
CUDA_VISIBLE_DEVICES="0,1,2,3"       # GPUs to use (minimum 4x A100 80GB or 8x A100 40GB)
MASTER_PORT="29500"                  # for distributed training
PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512"  # memory management
NCCL_P2P_DISABLE="1"                 # for better multi-GPU performance 